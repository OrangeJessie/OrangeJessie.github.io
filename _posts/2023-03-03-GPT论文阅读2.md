**GPT 论文阅读**

**1.Improving Language Understandingby Generative Pre-**

> **Training**

NLU
包含了很多任务，比如文字蕴含、QA、语义相似度评估、文本分类等，大量文本是
没有标签的，对特定任务有标签的数据很少，预训练的方法在 word embedding
方法下验证了
其有效性，但目前的预训练都是在词层面，因此本文用无标签数据训练**生成式预训练模型**，去
学习语言更高层的表示。GPT1
提出了一种新的方法来训练模型------**无监督的预训练+有监督的
精调**，模型结构采用 transformer，使模型能处理更长的文本。

> 限制：仍然需要精调，下游任务需要构造特定格式的输入数据。

**1.1 模型结构**

**1.1.1 无监督的 pre-train**

> 在无监督数据集上，用前 k 个词预测下一个词，目标是最大化似然函数

![](vertopal_ffff19b811c3417395d304b112508dee/media/image1.png){width="3.0in"
height="0.40138779527559054in"}

k 是文本窗口的大小，已知第 i 个词前的 k 个词的概率，求第 i
个词的概率，用随机梯度下 降来最大化似然函数。其中 P
为模型输出的概率，在本文中采用 multi-layer transformer decoder
作为语言模型，decoder 与 encoder 相比，self-attention 是
masked，此处采用 decoder 的原因
是生成式模型不能将后面的词暴露给前面的词。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image2.png){width="3.397221128608924in"
height="0.795832239720035in"}

其中![](vertopal_ffff19b811c3417395d304b112508dee/media/image3.png){width="1.2902777777777779in"
height="0.17083333333333334in"}是文本 token vector，n 是层数，We是
embedding 矩阵，Wp是位 置编码矩阵。

**1.1.2 有监督的 fine-tuning**

在这个阶段，针对有监督的数据集，用上一步中的 pre-train 模型去掉 softmax
层，计算输 入数据 x 的向量表示，再添加一层线性层来用输出向量预测标签 y

![](vertopal_ffff19b811c3417395d304b112508dee/media/image4.png){width="2.933333333333333in"
height="0.34999890638670167in"}

> 似然函数变为

![](vertopal_ffff19b811c3417395d304b112508dee/media/image5.png){width="2.6527777777777777in"
height="0.4791666666666667in"}

语言模型预训练在其中起的作用是 1. 泛化精调模型，2.
加速收敛。在特定情况下，优化目 标为 L1 和 L2，用 lambda
来调节预训练模型占比，这样能在不过多降低模型泛化能力的同时，增强在特定任务上的效果

![](vertopal_ffff19b811c3417395d304b112508dee/media/image6.png){width="2.2874989063867015in"
height="0.28194444444444444in"}

**1.1.3 任务的输入数据格式**

由于不同的任务输入数据格式是不同的，过去在模型最上层对特定的任务会使用特定的结
构。在本文中，将所有任务的输入数据都格式化为一个序列，这样就不需要再对模型做额外的
改动。下图左边是 pre-train 模型结构，右边是 fine-tuning
时数据输入的结构。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image7.png){width="6.298610017497813in"
height="3.1666666666666665in"}

**1.2 实验**

**1.2.1 数据集**

> pre-train 使用的数据集：
>
> 1\. 74 million
>
> 2\. 1 billion
>
> fine-tuning 任务使用的数据集：

![](vertopal_ffff19b811c3417395d304b112508dee/media/image8.png){width="6.298610017497813in"
height="1.3611111111111112in"}

**1.2.2 实验细节**

pre-train 和 fine-tuning 实验参数设置，在 fine-tuning
阶段没有额外说明的参数与 pre-train
阶段相同，此处参数并不完整，更详细的请参考论文。

+-----------------------+-----------------------+-----------------------+
| > 阶段                | > 参数                | > 设置                |
+=======================+=======================+=======================+
| > Pre-train           | > 模型结构            | > 12-layer            |
|                       |                       | > decoder-only        |
|                       |                       | > transformer         |
+-----------------------+-----------------------+-----------------------+
|                       | > 优化器              | > Adam                |
+-----------------------+-----------------------+-----------------------+
|                       | > 学习率              | > 前 2000 次更新从 0  |
|                       |                       | > 线性增加到最大      |
|                       |                       | > 2.5e-4              |
+-----------------------+-----------------------+-----------------------+
|                       | > 字典                | > Bytepair            |
|                       |                       | > encoding(BPE)       |
|                       |                       | > vocabulary          |
+-----------------------+-----------------------+-----------------------+
|                       | > MiniBatch           | > 100 epoches,        |
|                       |                       | > batch_size=64       |
|                       |                       | > randomly sampled    |
+-----------------------+-----------------------+-----------------------+
|                       | > 激活函数            | > Gaussian Error      |
|                       |                       | > Linear Unit (GELU)  |
+-----------------------+-----------------------+-----------------------+
|                       | > 正则                | > L2，with w= 0.01on  |
|                       |                       | > all non bias or     |
|                       |                       | > gain weights        |
+-----------------------+-----------------------+-----------------------+
|                       | > 参数量              | > 117 million         |
+-----------------------+-----------------------+-----------------------+
| > Fine-tuning         | > Dropout             | > 分类任务中设置为    |
|                       |                       | > 0.1                 |
+-----------------------+-----------------------+-----------------------+
|                       | > 学习率              | > 大部分任务 6.25e-5, |
|                       |                       | > decay               |
+-----------------------+-----------------------+-----------------------+
|                       | > Batch size          | > 32, 3 epoches       |
+-----------------------+-----------------------+-----------------------+

**2.Language Models are Unsupervised Multitask Learners**

语言模型能从无监督的数据中学习，应用到多种下游任务中，而不需要再进行精调，模型
量级对于提升 zero-shot
效果很重要。机器学习的通常方法是，用成千上万的训练数据去训练
模型，让模型得到泛化性，同理，如果要让语言模型在多任务上得到很好的泛化性，那么需要
更多的(dataset, objective)对。本文发现语言模型在 zero-shot
时，能在下游任务上表现很好。

训练方法是给语言模型多种 task 和 input，输出 output，建模为
p(output\|input, task)，举 个例子：翻译(translate tofrench, english
text, french text)，回答问题 (answer the question, document,question,
answer)。

限制：下游任务的输入数据格式经过一定的设计，但不包含 GPT-1
中的特殊字符，而是 用更接近自然语言的方式。

**2.1 训练数据**

> 1\. 在 Reddit 网站上网友评分 3 karma 以上的网络数据 Common Craw
>
> 2\. 8 million documents，40 GB 文本数据，移除 Wikipedia

**2.2 输入表示**

采用 BPE(Byte Pair Encoding)编码方式，采用 byte-level
BPE，除了空格，不允许跨 character merge(比如 dog! dog.)。BPE
将常见的词用整体表示，而罕见词用子结构表示。

> 优点：
>
> 对于罕见词，不需要用\<ukn\>符号来表示，而可以在词表中找到子符号。
>
> 不允许跨符号 merge 增加了压缩效率

**2.3 模型**

模型上沿用了 GPT-1，做了一些微改，论文中没有很具体地说明。文本长度从 512
增加到 1024，batch 大小增加到 512，在参数上比 GPT-1 多了一个量级。

**2.4 实验**

在多个下游任务上做 zero-shot
测试，包括语言模型、阅读理解、总结、翻译、问答上都 取得了超过 sota
的效果。以下仅举例部分实验。

**2.4.1 语言模型**

![](vertopal_ffff19b811c3417395d304b112508dee/media/image9.png){width="6.298610017497813in"
height="1.8055555555555556in"}

> 指标含义：

PPL 是在已知字符集的情况下，预测句子是怎么样的概率，字符集可以是
character，也可 以是 word，PPL 越小，模型效果越好。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image10.png){width="3.563888888888889in"
height="0.4388888888888889in"}

> BPC(bits per character)和 BPB(bits per byte)是用来衡量压缩率的指标。
> 压缩率为![](vertopal_ffff19b811c3417395d304b112508dee/media/image11.png){width="3.1486111111111112in"
> height="0.18611001749781278in"}
>
> BPB
> 是![](vertopal_ffff19b811c3417395d304b112508dee/media/image12.png){width="2.425in"
> height="0.16111111111111112in"}

与 BPC 相等。

> ，对于 ASCII Extended characters 来说，BPB

LAMBADA 测试模型对长文本依赖关系的建模能力，CBT 测试模型对 10
类不同词的区

分，包括名词、实体、动词等。

**2.4.2 常识推断**

![](vertopal_ffff19b811c3417395d304b112508dee/media/image13.png){width="4.0638877952755905in"
height="3.4972222222222222in"}

**2.5 Code**\
模型开源：

**3.Language Models are Few-Shot Learners**

> GPT-3 是一个 175billion 参数的自回归模型。Few-shot
> 的意思是，对于所有的任务，GPT-

3 能够不做精调和梯度更新，只需要提供一些样例。

**3.1 Introduction**\
介绍了为什么本文要采用 Few-shot
的测试方式，以及随着模型规模的增大，模型效果的

变化。

**3.1.1 实验发现**

> 随着模型规模和给出的样例增大，能很大程度地提升模型效果，尤其是在 175B
> 模型上。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image14.png){width="6.3in"
height="3.6041666666666665in"}

随着模型规模增大，三种不同设定下能达到的效果，可以观察到一个有趣的
pattern，随 着模型量级增大，few-shot，one-shot，和 zero-shot 之间的 gap
也增大了，可能说明更大的 模型是更好的学习者。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image15.png){width="6.3in"
height="3.4569444444444444in"}

**3.1.2 Few-shot 和 Fine-tuning 对比**

Fine-tuning
的优点在于能够通过精调来在特定任务上获得很好的效果，缺点在于，1. 下
游任务需要收集大量标注数据，2.
模型对于与训练集分布不同的数据泛化能力差，3. 模型很可

能利用数据集中虚假的特征，4.
在与人类效果对比时并不公平，因为精调使用了大量训练集中 的数据。

Few-shot
的优点在于下游任务需要的标注数据大大减少，并且泛化能力更好，但是目前
few-shot 还不能达到精调的最优效果。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image16.png){width="6.3in"
height="5.534722222222222in"}

**3.2 模型结构**

GPT-3 模型与 GPT-2 一样，训练了不同大小的模型，175B
的模型与其他模型相比，Batch size 增大，层数增多，head
数增加了一倍多。Learning rate 随模型增大降低，Batch size 随模
型增大而增大。dmodel是 bottleneck 层的 unit 数，所有模型的 context
window 都是\
2048tokens。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image17.png){width="6.3in"
height="2.044443350831146in"}

**3.3 训练数据**

数据集，是互联网爬取数据，原始数据集大小为 1 千亿词，但是质量参差
不齐，因此采用优化：

> 1.用 GPT-2 中的高质量数据集 WordText、Wikipedia 等作为正样本，Common
> Crawl 作 为负样本，训练一个逻辑回归分类模型，在 Common Crawl
> 中根据模型打分来采样训 练数据。random.pareto(9)\>1-document score
> 时，将样本采样到 dataset
>
> 2.模糊去重，减少冗余，以避免 overfitting。去除一些 high overlap
> 的文档，并将 WordText 从 Common Crawl 中去掉。降低了 10%的数据集大小
>
> 3.加入其他高质量数据集，提升数据多样性。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image18.png){width="6.3in"
height="2.025in"}

在训练过程中，Common Crawl
被采样的概率比其他高质量数据更低，尽管数据量大很
多，但是实际占比训练数据 60%。

**3.4 结果**

参数量更大的模型能够在验证集上达到更小 loss，在计算前期，模型 loss
快速下降，计算 量增大两个数量级后，模型效率降低。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image19.png){width="6.3in"
height="3.8375in"}

> 在各种语言任务下的效果，仅举例部分：
>
> Lambada 给定前面的句子，预测最后一个 word。Few-shot
> 对效果有很明显的增强作用。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image20.png){width="6.298610017497813in"
height="3.5652777777777778in"}

> 在 TriviaQA 任务中，Few-shot 的模型超过了 Fine-tuned 的 sota 模型。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image21.png){width="6.3in"
height="3.5388888888888888in"}

> Winogrande，判断代词指的是哪个词的任务。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image22.png){width="6.298610017497813in"
height="3.3375in"}

**3.5 Limitation**

1.在文本合成和一些 nlp 任务上表现不足够好。

2.在预训练时样本效率低，预训练仍然需要大量的文本数据。

3.Few-shot 并不确定是真的学到了新任务还是识别到了已知的任务。

4.Inference 昂贵。

5.对于训练集中没有出现过的文本，不能很好地处理，比如 code。

**4.Training language models to follow instructions with**

> **human feedback**

语言模型量级变得更大并不会使模型更理解用户的意图，模型生成的内容可能是没用的，有毒的或者虚假的。这是因为模型训练的目标是预测下一个
token，与 follow 用户指令不相 同。

本文的研究表明，用人类反馈来精调模型，能使模型变得更理解用户意图。

> 主要的发现：\
> 1.相比 175B 的 GPT-3，标注员更喜欢 1.3B 的 InstructGPT 的输出；\
> 2.InstructGPT 真实性有提升，在 close-domain
> 任务中更少编造输入中没有的信息； 3.减少了有毒的输出，但没有减少偏见；\
> 4.融合原始模型目标和精调目标(PPO-ptx)，减少模型原始目标效果降低(各种公开数据集
> 效果)；\
> 5.InstructGPT 对训练集中没有出现过的指令有泛化能力，能 follow
> 不同语言的指令； 6.InstructGPT 仍然会犯简单的错误。

**4.1 模型**

> 训练过程分为 3 步

![](vertopal_ffff19b811c3417395d304b112508dee/media/image23.png){width="6.3in"
height="4.531944444444444in"}

**4.1.1 SFT**

从 prompt 集中采样一些 prompt，标注员根据 prompt 写出回答用于精调 SFT
模型，目标 是使生成的句子概率最大。

**4.1.2 RM**

对同一个
prompt，用模型生成多个回答，标注员对从中采样的回答进行排序，用生成的
回答和排序的结果作为训练数据，训练一个 Reward Model。RM 用了 6B
的模型，节省资源且 175B 模型不稳定。

采用对比学习的方式，一个 prompt 生成
K(4-9)个回答，标注员两两对比好坏（读懂题意
需要时间，可以生成更多答案降低平均每个排序的时间成本），将一个 prompt
生成的所有样本 对 CK 2放在一个 batch
中训练，防止过拟合（如果将每一个样本对作为一个单独的数据训练，会使每个回答影响梯度更新
K-1 次，而在一个 batch 里只会影响一次梯度更新）。

> 损失函数采用交叉熵

![](vertopal_ffff19b811c3417395d304b112508dee/media/image24.png){width="6.2972222222222225in"
height="1.05in"}

**4.1.3 PPO**

用强化学习 算法精调 SFT，随机采样的 prompt，模型生成一个回复，将 prompt
和 回复用 RM 得到 reward。RL 模型是从 SFT
初始化而来，目标函数第一项是最大化 reward 值，第二项控制 RL 分布和 SFT
分布差异，第三项融合精调梯度，使模型不会在原始数据集上效果 变差过多。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image25.png){width="6.298610017497813in"
height="1.5638888888888889in"}

**4.2 数据集生成**

采样 prompt 时，去重了有很长的重复子串的 prompt，且对一个
user_id，限制最多采样 200 条，并根据 user_id
来划分了训练集、验证集和测试集。

SFT 数据含有 13k 条训练 prompt 数据：1.
标注员随意想的任务和回答，保证任务的多样 性；2.
标注员想一些指令性的问题和回答 pair；3.
标记员根据用户真实例子想一些类似的 prompt。

> RM 含有 33k 训练 prompt 数据，PPO 含有 31k 训练 prompt 数据。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image26.png){width="1.7555555555555555in"
height="2.245832239720035in"}

> 数据集中超过 96%是英语，但是仍然能处理其他语言的指令。
>
> 文章中还提到了如何手机人类数据的细节，如果有标注需求可以参考。

**4.3 评估方法**

在评估模型效果时，目标是 Helpful、Honest、Harmless。Helpful
通过标注员标注来评 价，但是和用户真正的意图是有偏差的。Honest 通过完成
lose-domain 的任务和 TruthfulQA 来评估。Harmless 通过标注员标注
potential harmful 来评价。

**4.4 结果**

**4.4.1 API prompt**

评价指标是与 SFT 175B 模型相比的胜率，GPT-3 效果最差，但是通过 prompt
可以显著提 高效果。经过 SFT 和 PPO 的 1.3B 模型效果好于 GPT-3
175B，小模型通过精调和强化学习能在 特定领域超过大模型的表现。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image27.png){width="6.3in"
height="3.8430555555555554in"}

下图左边是提交给 GPT-3 的 prompt 集上的结果，右边是提交到 InstructGPT 的
prompt 集 上的结果，上面是 held-out labeler 标注，下面是参与训练集的
labeler 标注。

在同样的 prompt 集上，SFT 在 Training worker 比在 Heldout worker
上表现更好，PPO 虽 然对这种现象有所改善，但还是存在。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image28.png){width="6.3in"
height="6.106944444444444in"}

在有明确指令的任务和遵循显式约束的任务中，模型取得了较大的提升，减少了虚假信
息。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image29.png){width="6.298610017497813in"
height="2.929165573053368in"}

**4.4.2 公开 NLP 数据集**

InstructGPT 在真实性上有明显提升，三个柱状从左到右分别是模型规模
1.3B、6B、175B。PPO-ptx 1.3B 模型效果差于 GPT-3 1.3B，其他情况下
InstructGPT 都相比 GPT-3 有提 升。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image30.png){width="6.3in"
height="3.2041666666666666in"}

InstructGPT 在有毒性上降低了，但在有偏性上没有显著变化。当被指示
respectful 和 safe 时，InstructGPT 比 GPT-3
表现更好一些，但是在没有指示的时候，没有明显提升。

![](vertopal_ffff19b811c3417395d304b112508dee/media/image31.png){width="6.3in"
height="3.9763888888888888in"}
